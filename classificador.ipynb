{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo funções para construção de treino e testes do dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    data = fetch_20newsgroups(\n",
    "        subset='all', \n",
    "        shuffle=True, \n",
    "        remove=('headers', 'footers', 'quotes'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(corpus, labels, test_data_proportion=0.3):\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(\n",
    "        corpus, \n",
    "        labels,\n",
    "        test_size=0.33,\n",
    "        random_state=42)\n",
    "    return train_X, test_X, train_Y, test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_docs(corpus, labels):\n",
    "    filtered_corpus = []\n",
    "    filtered_labels = []\n",
    "    for doc, label in zip(corpus, labels):\n",
    "        if doc.strip():\n",
    "            filtered_corpus.append(doc)\n",
    "            filtered_labels.append(label)\n",
    "    return filtered_corpus, filtered_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisando dados do meu dataset como seus documentos e classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_data()\n",
    "classes = dataset.target_names\n",
    "\n",
    "corpus, labels = dataset.data, dataset.target \n",
    "corpus, labels = remove_empty_docs(corpus, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Amostra do Documento:', u'the blood of the lamb.\\n\\nThis will be a hard task, because most cultures used most animals\\nfor blood sacrifices. It has to be something related to our current\\npost-modernism state. Hmm, what about used computers?\\n\\nCheers,\\nKent')\n",
      "('Indice da Classe:', 19)\n",
      "('classe do indice atual', 'talk.religion.misc')\n"
     ]
    }
   ],
   "source": [
    "print('Amostra do Documento:', corpus[10])\n",
    "print('Indice da Classe:',labels[10])\n",
    "print('classe do indice atual', dataset.target_names[labels[10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparando dados de treino e teste do meu dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus, test_corpus, train_labels, test_labels = prepare_datasets(\n",
    "                                                            corpus, \n",
    "                                                            labels, \n",
    "                                                            test_data_proportion=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando funcionalidades de feature extractor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_bow = CountVectorizer()\n",
    "vect_tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraindo os recursos de Bag Of Words e TF-IDF dos meus docs e classes de teste e treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of Words:\n",
    "X_train_bow = vect_bow.fit_transform(train_corpus)\n",
    "X_test_bow= vect_bow.transform(test_corpus)\n",
    "#TF-IDF\n",
    "X_train_tfidf = vect_tfidf.fit_transform(train_corpus)\n",
    "X_test_tfidf = vect_tfidf.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de extrair os recusros, crio uma função para avaliar nossa classificação de acordo com \n",
    "as metricas: accuary, precision, recall e f1score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(true_labels, predicted_labels):\n",
    "    print ('Accuracy:', np.round(metrics.accuracy_score(true_labels,predicted_labels),2))\n",
    "    print ('Precision:', np.round(metrics.precision_score(true_labels,predicted_labels,average='weighted'),2))\n",
    "    print ('Recall:', np.round(metrics.recall_score(true_labels,predicted_labels,average='weighted'),2))\n",
    "    print ('F1 Score:', np.round(metrics.f1_score(true_labels,predicted_labels,average='weighted'),2))\n",
    "    print(\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para treinar o modelo com o classificador desejado, realizar predição e resgatar as metricas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict_evaluate_model(classifier,train_features, train_labels,test_features, test_labels):  \n",
    "    classifier.fit(train_features, train_labels)\n",
    "    predictions = classifier.predict(test_features) \n",
    "    # avaliar o desempenho de previsão do modelo \n",
    "    get_metrics(true_labels=test_labels,predicted_labels=predictions)\n",
    "    return predictions  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando classificadores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "svm = SGDClassifier(loss='hinge', max_iter=1000, tol=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes e SVM com recursos do bow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy:', 0.62)\n",
      "('Precision:', 0.71)\n",
      "('Recall:', 0.62)\n",
      "('F1 Score:', 0.6)\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=X_train_bow,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=X_test_bow,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy:', 0.66)\n",
      "('Precision:', 0.68)\n",
      "('Recall:', 0.66)\n",
      "('F1 Score:', 0.66)\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=X_train_bow,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=X_test_bow,\n",
    "                                           test_labels=test_labels)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes e SVM com recursos do TFIDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy:', 0.68)\n",
      "('Precision:', 0.78)\n",
      "('Recall:', 0.68)\n",
      "('F1 Score:', 0.67)\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mnb_tfidf_predictions = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=X_train_tfidf,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=X_test_tfidf,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy:', 0.77)\n",
      "('Precision:', 0.77)\n",
      "('Recall:', 0.77)\n",
      "('F1 Score:', 0.77)\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "svm_tfidf_predictions = train_predict_evaluate_model(classifier=svm,\n",
    "                                           train_features=X_train_tfidf,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=X_test_tfidf,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisando documentos que foram classificados incorretamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2    3    4    5    6    7    8    9    10   11   12   13   14  \\\n",
      "0   144    2    0    1    0    2    2    2    6    1    7    5    5    6    6   \n",
      "1     0  226    9    5    6   17    6    0    3    1    1    2    3    5    7   \n",
      "2     1   15  222   19   13   16    6    2    0    1    1    1    7    4    3   \n",
      "3     1   12   21  217   17    5   11    3    0    3    1    2    4    1    0   \n",
      "4     0    6    5   17  228    3    4    3    2    2    0    3   12    2    2   \n",
      "5     0   23   19    2    0  275    1    0    0    0    0    0    1    2    1   \n",
      "6     0    1    5   12   12    2  273   11    3    4    0    1    9    2    2   \n",
      "7     2    4    2    2    4    2   12  242   19    2    2    0    9    3    2   \n",
      "8     2    1    2    2    2    0    5   26  262    5    1    2    2    1    2   \n",
      "9     1    0    2    0    1    2    5    3    2  286   11    0    0    2    2   \n",
      "10    0    0    0    0    1    1    0    2    1    7  280    1    1    1    0   \n",
      "11    3    4    4    3    1    3    1    0    2    2    0  256    9    4    0   \n",
      "12    0    4    5   15    9    0   15   10    5    2    3    3  215    4    7   \n",
      "13    3    2    0    0    2    2    2    0    1    1    0    0    7  274    4   \n",
      "14    1    5    1    0    1    2    5    6    2    4    1    2    3    6  265   \n",
      "15   13    2    0    0    1    1    2    2    1    5    1    3    1    7    4   \n",
      "16    6    1    1    0    2    3    2    1    6    6    4    7    3    4    5   \n",
      "17    3    1    0    0    1    5    2    1    2    2    4    2    0    3    0   \n",
      "18    5    1    0    2    0    0    1    2    4    6    2    8    0   13    7   \n",
      "19   19    3    0    1    0    4    3    3    4    3    3    1    0   10    2   \n",
      "\n",
      "     15   16   17   18  19  \n",
      "0    38    4    7    9  16  \n",
      "1     2    2    0    2   0  \n",
      "2     0    1    0    2   0  \n",
      "3     0    1    1    0   0  \n",
      "4     1    1    0    1   0  \n",
      "5     0    0    1    0   0  \n",
      "6     0    1    1    0   0  \n",
      "7     2    3    1    1   2  \n",
      "8     1    3    1    2   0  \n",
      "9     0    3    1    2   2  \n",
      "10    0    1    2    4   1  \n",
      "11    3    3    2    6   0  \n",
      "12    2    1    1    0   1  \n",
      "13    3    3    2    1   1  \n",
      "14    3    4    0    3   0  \n",
      "15  285    2    4    4   5  \n",
      "16    3  221    4   10   2  \n",
      "17    5    5  272    4   3  \n",
      "18    5   34   11  158   3  \n",
      "19   61   18    9    7  62  \n"
     ]
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(test_labels, svm_tfidf_predictions)\n",
    "print(pd.DataFrame(cm, index=range(0,20), columns=range(0,20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('alt.atheism', '->', 'soc.religion.christian')\n",
      "('talk.politics.misc', '->', 'talk.politics.guns')\n",
      "('talk.religion.misc', '->', 'soc.religion.christian')\n"
     ]
    }
   ],
   "source": [
    "class_names = dataset.target_names\n",
    "\n",
    "print (class_names[0], '->', class_names[15])\n",
    "print (class_names[18], '->', class_names[16]) \n",
    "print (class_names[19], '->', class_names[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
